{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script to process the comments, extract sentiment and make it into a decision class/verdict\n",
    "\n",
    "This script processes only the comments and is based on sentiment analysis of NLP. Basic version, using just the keywords. \n",
    "\n",
    "Processing of comments:\n",
    "* find comments that are positive \n",
    "* find comments that are negative\n",
    "* finding = looking for specific word and phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lexicon for positive sentiments\n",
    "keywordsComments_positive = ['good', 'idea', 'good idea', \n",
    "                             'done', 'beside', 'improved', \n",
    "                             'thank' 'yes', 'well', \n",
    "                             'nice', 'positive', 'better', \n",
    "                             'best', 'super', 'great', \n",
    "                             'fantastic']\n",
    "\n",
    "# lexicon for negative sentiments \n",
    "keywordsComments_negative = ['not good', 'not improve', \"don't\"\n",
    "                             'should', 'should not', '?', \n",
    "                             'aside', 'tend', 'not done', \n",
    "                             'bad', 'improve', 'remove', \n",
    "                             'add', 'include', 'not include', \n",
    "                             'defeat', 'no', 'do not',  \n",
    "                             'chaotic', 'negative', 'worse', \n",
    "                             'worst']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# location of the file with the input\n",
    "filename = './gerrit_reviews_wireshark.csv'\n",
    "\n",
    "# location of the file with the output feature vector\n",
    "saveFilename = './gerrit_review_comments_dictionary_sentiment_wireshark.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy\n",
    "# we use it to easily work with arrays\n",
    "import numpy as np\n",
    "\n",
    "# we use it for saving CSV file\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# comment feature vector is analyzed based on the keywords specified as parameters\n",
    "# the sentiment is based on the percentage of positive - negative keywords found\n",
    "def comment2sentiment(strComment,keywordsComments_positive, keywordsComments_negative):\n",
    "    countPositive = 0\n",
    "    countNegative = 0\n",
    "    \n",
    "    totalPositives = len(keywordsComments_positive)\n",
    "    totalNegatives = len(keywordsComments_negative)\n",
    "    \n",
    "    for oneKeyword in keywordsComments_positive:\n",
    "        countPositive += strComment.lower().count(oneKeyword.lower())\n",
    "    \n",
    "    for oneKeyword in keywordsComments_negative:\n",
    "        countNegative += strComment.lower().count(oneKeyword.lower())\n",
    "    \n",
    "    quotinentPositive = countPositive / totalPositives\n",
    "    quotinentNegative = countNegative / totalNegatives\n",
    "    \n",
    "    sentimentQuotinent = quotinentPositive - quotinentNegative\n",
    "    \n",
    "    # once we have the quotinent, we change it into verdict\n",
    "    # anything that is positive becomes 1 and\n",
    "    # anything that is negative becomes 0\n",
    "    if sentimentQuotinent > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "INFO: Skipping Commit message in line /COMMIT_MSG: It is indicated by adding a \"[Tree view truncated]\" item.\nINFO: Lines processed: 11\nINFO: Skipping Commit message in line /COMMIT_MSG: \nINFO: Lines processed: 11\nINFO: Skipping Commit message in line /COMMIT_MSG: Tested-by: Petri Dish Buildbot\nINFO: Lines processed: 11\nINFO: Skipping Commit message in line /COMMIT_MSG: Reviewed-by: Peter Wu <peter@lekensteyn.nl>\nINFO: Lines processed: 97\nINFO: Skipping Commit message in line /COMMIT_MSG:   value while in fact it is 32-bit wide\nINFO: Lines processed: 106\nINFO: Skipping Commit message in line /COMMIT_MSG: Reviewed-by: Tomasz Moń <desowin@gmail.com>\nINFO: Lines processed: 115\nINFO: Skipping Commit message in line /COMMIT_MSG: Change-Id: Ie8b005ccadda39c653782fc38280ce21cf2ca0a8\nINFO: Lines processed: 141\nINFO: Skipping Commit message in line /COMMIT_MSG: In this release (IEC 61850-7-2:2003) there is a field name called: Test.\nINFO: Lines processed: 158\nINFO: Skipping Commit message in line /COMMIT_MSG: \nINFO: Lines processed: 158\nINFO: Skipping Commit message in line /COMMIT_MSG: Reviewed-on: https://code.wireshark.org/review/36091\nINFO: Lines processed: 194\nINFO: Skipping Commit message in line /COMMIT_MSG: \nINFO: Lines processed: 205\nINFO: Skipping Commit message in line /COMMIT_MSG: \nINFO: Lines processed: 218\nINFO: Skipping Commit message in line /COMMIT_MSG: \nINFO: Lines processed: 218\nINFO: Skipping Commit message in line /COMMIT_MSG: \nINFO: Lines processed: 218\nINFO: Skipping Commit message in line /COMMIT_MSG: \nINFO: Lines processed: 218\nINFO: Skipping Commit message in line /COMMIT_MSG: Fixes: v3.3.0rc0-461-g8efde39805 (\"extcap: terminate the child process using kill.\")\nINFO: Lines processed: 362\nINFO: Skipping Commit message in line /COMMIT_MSG: \nINFO: Lines processed: 377\nINFO: Skipping Commit message in line /COMMIT_MSG: \nINFO: Lines processed: 419\n"
    }
   ],
   "source": [
    "# use this to skip the first line and to print out something once every 1000 lines\n",
    "iIndex = 0\n",
    "\n",
    "# initializing a data frame with the result of the sentiment analysis\n",
    "dfSentimentedLines = pd.DataFrame()\n",
    "\n",
    "with open(filename, 'r', encoding = 'utf-8') as fInputFile:\n",
    "    for strInputLine in fInputFile:\n",
    "        lineElements = strInputLine.split(';')\n",
    "        iIndex += 1\n",
    "        \n",
    "        if not iIndex % 1000:\n",
    "            print(f'INFO: Processing line {iIndex}') \n",
    "        \n",
    "        if len(lineElements) > 7 and iIndex > 1:\n",
    "            strLineCode = lineElements[6]\n",
    "            strLineComment = lineElements[7].replace('\\n', '_')\n",
    "            strReviewFilename = lineElements[2]\n",
    "            \n",
    "            # filter if line is about COMMIT_MSG\n",
    "            # if not, then we calculate the sentiment\n",
    "            if not 'COMMIT_MSG' in strReviewFilename:                \n",
    "                sentiment = comment2sentiment(strLineComment, keywordsComments_positive, keywordsComments_negative) \n",
    "                oneRow = {'filename': strReviewFilename, 'LOC': strLineCode, 'class_value': sentiment, 'message': strLineComment}\n",
    "                dfSentimentedLine = pd.DataFrame([oneRow], columns = oneRow.keys())\n",
    "                dfSentimentedLines = pd.concat([dfSentimentedLines, dfSentimentedLine], axis=0)\n",
    "            else:\n",
    "                print(f'INFO: Skipping Commit message in line {strReviewFilename}: {strLineCode}')\n",
    "                print(f'INFO: Lines processed: {dfSentimentedLines.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>filename</th>\n      <th>LOC</th>\n      <th>class_value</th>\n      <th>message</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>.github/workflows/close_pr.yml</td>\n      <td>comment: \"We do not accept PRs. Patche...</td>\n      <td>0</td>\n      <td>Maybe \"We do not accept GitHub PRs.\" ?_</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>epan/dissectors/packet-tls-utils.c</td>\n      <td>tvb, offset, next_offset -...</td>\n      <td>0</td>\n      <td>since you are not adding any extra information...</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>epan/dissectors/packet-tls-utils.c</td>\n      <td>tvb, offset, next_offset -...</td>\n      <td>1</td>\n      <td>Done_</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>epan/dissectors/packet-tls-utils.c</td>\n      <td>tvb, offset, next_offset -...</td>\n      <td>0</td>\n      <td>Would this be worth showing as expert info ins...</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>epan/dissectors/packet-tls-utils.c</td>\n      <td>tvb, offset, next_offset -...</td>\n      <td>0</td>\n      <td>The number of layers added per DN is not fixed...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>packaging/rpm/wireshark.spec.in</td>\n      <td>* Sun Jan 19 2019 Jiri Novak</td>\n      <td>0</td>\n      <td>Should not it be Sun Jan 19 2020 at the previo...</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>ui/io_graph_item.c</td>\n      <td>#define get_io_graph_item_advanced_FT_xINT_uni...</td>\n      <td>0</td>\n      <td>Using macros makes the code more compacet but ...</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>ui/io_graph_item.c</td>\n      <td>//shift left to take the sign bit ...</td>\n      <td>0</td>\n      <td>Typo in comment: \"durch\" should be \"during\"_</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>ui/io_graph_item.c</td>\n      <td>//shift left to take the sign bit ...</td>\n      <td>0</td>\n      <td>Is this translation needed for unsigned quanti...</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>ui/io_graph_item.c</td>\n      <td>//undo the left shift by dividing ...</td>\n      <td>0</td>\n      <td>Is this translation needed for unsigned quanti...</td>\n    </tr>\n  </tbody>\n</table>\n<p>421 rows × 4 columns</p>\n</div>",
      "text/plain": "                              filename  \\\n0       .github/workflows/close_pr.yml   \n0   epan/dissectors/packet-tls-utils.c   \n0   epan/dissectors/packet-tls-utils.c   \n0   epan/dissectors/packet-tls-utils.c   \n0   epan/dissectors/packet-tls-utils.c   \n..                                 ...   \n0      packaging/rpm/wireshark.spec.in   \n0                   ui/io_graph_item.c   \n0                   ui/io_graph_item.c   \n0                   ui/io_graph_item.c   \n0                   ui/io_graph_item.c   \n\n                                                  LOC  class_value  \\\n0           comment: \"We do not accept PRs. Patche...            0   \n0                       tvb, offset, next_offset -...            0   \n0                       tvb, offset, next_offset -...            1   \n0                       tvb, offset, next_offset -...            0   \n0                       tvb, offset, next_offset -...            0   \n..                                                ...          ...   \n0                        * Sun Jan 19 2019 Jiri Novak            0   \n0   #define get_io_graph_item_advanced_FT_xINT_uni...            0   \n0               //shift left to take the sign bit ...            0   \n0               //shift left to take the sign bit ...            0   \n0               //undo the left shift by dividing ...            0   \n\n                                              message  \n0             Maybe \"We do not accept GitHub PRs.\" ?_  \n0   since you are not adding any extra information...  \n0                                               Done_  \n0   Would this be worth showing as expert info ins...  \n0   The number of layers added per DN is not fixed...  \n..                                                ...  \n0   Should not it be Sun Jan 19 2020 at the previo...  \n0   Using macros makes the code more compacet but ...  \n0        Typo in comment: \"durch\" should be \"during\"_  \n0   Is this translation needed for unsigned quanti...  \n0   Is this translation needed for unsigned quanti...  \n\n[421 rows x 4 columns]"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfSentimentedLines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the output into a .csv file with $ as separator\n",
    "pd.DataFrame(dfSentimentedLines).to_csv(saveFilename, \n",
    "                                        sep = \"$\",\n",
    "                                        index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The results of this script are saved now in a .csv file where each line is tagged with the sentiment-analyzed verdict. It can be used as a dictionary of \"verdict\" for each line. The file is in a raw format, which means:\n",
    "* it contains duplicated lines - some lines can duplicated with a different verdict\n",
    "* it contains mny duplicated lines - many lines are naturally part of many commits and sometimes even in the same commit we could have extracted them twice (sometimes the API provides us with the same data)\n",
    "* it contains irrelevant lines - some lines can be like \"#\" or \"//\" or even \"\" only, which means that we need to clean up the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}