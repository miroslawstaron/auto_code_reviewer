{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Code featurizer\n",
    "\n",
    "This workbook analyzes the lines of code using the bag-of-words algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# a list of the delimiters in the code, extra stop words\n",
    "# source: most common programming languages\n",
    "CODE_STOP_DELIM = \"([\\s\\t\\(\\)\\[\\]{}!@#$%^&*\\/\\+\\-=;:\\\\\\\\|`'\\\"~,.<>/?\\n])\"\n",
    "\n",
    "# number of words in the resulting BoW dictionary\n",
    "number_of_words = 1000\n",
    "\n",
    "# input filename\n",
    "input_filename = './gerrit_review_comments_dictionary_sentiment_wireshark.csv'\n",
    "\n",
    "# save filename\n",
    "save_filename = './gerrit_review_for_classifier_wireshark.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BoW tokenizer\n",
    "def code_stop_words_tokenizer(line):\n",
    "    global CODE_STOP_DELIM\n",
    "    split_line = re.split(CODE_STOP_DELIM, line)\n",
    "    split_line = list(filter(lambda a: a != '', split_line))\n",
    "    split_line = [\"0\" if x.isdigit() else x for x in split_line]\n",
    "    return split_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = [\"temp = a;\", \"a = b;\", \"b = temp;\"]\n",
    "\n",
    "\n",
    "# reading in the lines as a list\n",
    "inputDF = pd.read_csv(input_filename, sep='$')\n",
    "\n",
    "# converting the data frame to a list, and filling in np.nan with ''\n",
    "linesList = list(inputDF['LOC'].fillna(''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counting the number of tokens in the file\n",
    "count_vect = CountVectorizer(max_features=number_of_words, \n",
    "                             tokenizer=code_stop_words_tokenizer, \n",
    "                             ngram_range=(1,1))\n",
    "\n",
    "# creating the bag of words vectors\n",
    "bag_of_words = count_vect.fit_transform(linesList).todense()\n",
    "\n",
    "# column names are the BoW tokens based on frequency\n",
    "colnames = [x for x in sorted(count_vect.vocabulary_.keys())]\n",
    "\n",
    "# create a data frame based\n",
    "lines_bow = pd.DataFrame(bag_of_words, columns=colnames)\n",
    "\n",
    "# concatenate the lines and their corresponding BoW representation into a single data frame\n",
    "linesCommentsDF = pd.concat([inputDF, lines_bow], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>filename</th>\n      <th>LOC</th>\n      <th>class_value</th>\n      <th>\\t</th>\n      <th></th>\n      <th>!</th>\n      <th>\"</th>\n      <th>#</th>\n      <th>$</th>\n      <th>%</th>\n      <th>...</th>\n      <th>wsapp</th>\n      <th>wsdg_html_chunked</th>\n      <th>www</th>\n      <th>xml</th>\n      <th>y</th>\n      <th>you</th>\n      <th>zstd</th>\n      <th>{</th>\n      <th>|</th>\n      <th>}</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>.github/workflows/close_pr.yml</td>\n      <td>comment: \"We do not accept PRs. Patche...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>21</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>epan/dissectors/packet-tls-utils.c</td>\n      <td>tvb, offset, next_offset -...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>25</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>epan/dissectors/packet-tls-utils.c</td>\n      <td>tvb, offset, next_offset -...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>25</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>epan/dissectors/packet-tls-utils.c</td>\n      <td>tvb, offset, next_offset -...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>25</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>epan/dissectors/packet-tls-utils.c</td>\n      <td>tvb, offset, next_offset -...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>25</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 636 columns</p>\n</div>",
      "text/plain": "                             filename  \\\n0      .github/workflows/close_pr.yml   \n1  epan/dissectors/packet-tls-utils.c   \n2  epan/dissectors/packet-tls-utils.c   \n3  epan/dissectors/packet-tls-utils.c   \n4  epan/dissectors/packet-tls-utils.c   \n\n                                                 LOC  class_value  \\t      !  \\\n0          comment: \"We do not accept PRs. Patche...            0   0  21  0   \n1                      tvb, offset, next_offset -...            0   0  25  0   \n2                      tvb, offset, next_offset -...            1   0  25  0   \n3                      tvb, offset, next_offset -...            0   0  25  0   \n4                      tvb, offset, next_offset -...            0   0  25  0   \n\n   \"  #  $  %  ...  wsapp  wsdg_html_chunked  www  xml  y  you  zstd  {  |  }  \n0  2  0  0  0  ...      0                  1    1    0  0    0     0  0  0  0  \n1  2  0  0  0  ...      0                  0    0    0  0    0     0  0  0  0  \n2  2  0  0  0  ...      0                  0    0    0  0    0     0  0  0  0  \n3  2  0  0  0  ...      0                  0    0    0  0    0     0  0  0  0  \n4  2  0  0  0  ...      0                  0    0    0  0    0     0  0  0  0  \n\n[5 rows x 636 columns]"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linesCommentsDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the output into a .csv file with $ as separator\n",
    "pd.DataFrame(linesCommentsDF).to_csv(save_filename, \n",
    "                                        sep = \"$\",\n",
    "                                        index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}